{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Index(['question', 'answer'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\M\\AppData\\Local\\Temp\\ipykernel_8484\\646550934.py:70: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "C:\\Users\\M\\AppData\\Local\\Temp\\ipykernel_8484\\646550934.py:88: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 3.280006\n",
      "Epoch 2, Average Loss: 1.335328\n",
      "Epoch 3, Average Loss: 0.606631\n",
      "Epoch 4, Average Loss: 0.432392\n",
      "Epoch 5, Average Loss: 0.275775\n",
      "Fine-tuning with mBERT completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# %pip install transformers\n",
    "# %pip install torch\n",
    "# %pip install pandas\n",
    "\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")  # Should print \"cuda\" if GPU is detected\n",
    "\n",
    "# Load dataset\n",
    "qa_data = pd.read_csv('../data/QA_greeting.csv')  # Update to your new dataset name\n",
    "print(qa_data.columns)  # Verify column names\n",
    "\n",
    "# Load mBERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-base-multilingual-cased\").to(device)\n",
    "\n",
    "# Define custom dataset for QA\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, questions, answers, tokenizer, max_length):\n",
    "        self.questions = questions\n",
    "        self.answers = answers\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        answer = self.answers[idx]\n",
    "        \n",
    "        # Tokenize question and answer\n",
    "        encoding = self.tokenizer(\n",
    "            question,\n",
    "            answer,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Identify the start and end positions of the answer within the input\n",
    "        answer_tokens = self.tokenizer.tokenize(answer)\n",
    "        input_tokens = self.tokenizer.tokenize(question) + [self.tokenizer.sep_token] + answer_tokens\n",
    "\n",
    "        start_position = input_tokens.index(answer_tokens[0])\n",
    "        end_position = start_position + len(answer_tokens) - 1\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"start_positions\": torch.tensor(start_position),\n",
    "            \"end_positions\": torch.tensor(end_position),\n",
    "        }\n",
    "\n",
    "# Prepare the dataset\n",
    "max_length = 128\n",
    "qa_dataset = QADataset(qa_data[\"question\"].tolist(), qa_data[\"answer\"].tolist(), tokenizer, max_length)\n",
    "data_loader = DataLoader(qa_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Optimizer and gradient scaler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training settings\n",
    "epochs = 5  # Adjust epochs for better performance\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        start_positions = batch[\"start_positions\"].to(device)\n",
    "        end_positions = batch[\"end_positions\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():  # Mixed precision\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                start_positions=start_positions,\n",
    "                end_positions=end_positions,\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss:.6f}\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save_pretrained('../model/qa_greeting_model_mbert')\n",
    "tokenizer.save_pretrained('../model/mbert_tokenizer')\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Fine-tuning with mBERT completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %pip install transformers\n",
    "# # %pip install torch\n",
    "# # %pip install pandas\n",
    "\n",
    "# from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "# import torch\n",
    "# import pandas as pd\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# from torch.optim import AdamW\n",
    "# from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# # Check if GPU is available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")  # Should print \"cuda\" if GPU is detected\n",
    "\n",
    "# # Load dataset\n",
    "# qa_data = pd.read_csv('../data/QA_greeting.csv')  # Update to your new dataset name\n",
    "# print(qa_data.columns)  # Verify column names\n",
    "\n",
    "# # Load mBERT model and tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "# model = BertForQuestionAnswering.from_pretrained(\"bert-base-multilingual-cased\").to(device)\n",
    "\n",
    "# # Define custom dataset for QA\n",
    "# class QADataset(Dataset):\n",
    "#     def __init__(self, questions, answers, tokenizer, max_length):\n",
    "#         self.questions = questions\n",
    "#         self.answers = answers\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_length = max_length\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.questions)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         question = self.questions[idx]\n",
    "#         answer = self.answers[idx]\n",
    "        \n",
    "#         # Tokenize question and answer\n",
    "#         encoding = self.tokenizer(\n",
    "#             question,\n",
    "#             answer,\n",
    "#             truncation=True,\n",
    "#             padding=\"max_length\",\n",
    "#             max_length=self.max_length,\n",
    "#             return_tensors=\"pt\",\n",
    "#         )\n",
    "\n",
    "#         # Identify the start and end positions of the answer within the input\n",
    "#         answer_tokens = self.tokenizer.tokenize(answer)\n",
    "#         input_tokens = self.tokenizer.tokenize(question) + [self.tokenizer.sep_token] + answer_tokens\n",
    "\n",
    "#         start_position = input_tokens.index(answer_tokens[0])\n",
    "#         end_position = start_position + len(answer_tokens) - 1\n",
    "\n",
    "#         return {\n",
    "#             \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "#             \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "#             \"start_positions\": torch.tensor(start_position),\n",
    "#             \"end_positions\": torch.tensor(end_position),\n",
    "#         }\n",
    "\n",
    "# # Prepare the dataset\n",
    "# max_length = 128\n",
    "# qa_dataset = QADataset(qa_data[\"question\"].tolist(), qa_data[\"answer\"].tolist(), tokenizer, max_length)\n",
    "# data_loader = DataLoader(qa_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# # Optimizer and gradient scaler\n",
    "# optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "# scaler = GradScaler()\n",
    "\n",
    "# # Training settings\n",
    "# epochs = 5  # Adjust epochs for better performance\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "\n",
    "#     for batch in data_loader:\n",
    "#         input_ids = batch[\"input_ids\"].to(device)\n",
    "#         attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#         start_positions = batch[\"start_positions\"].to(device)\n",
    "#         end_positions = batch[\"end_positions\"].to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         with autocast():  # Mixed precision\n",
    "#             outputs = model(\n",
    "#                 input_ids=input_ids,\n",
    "#                 attention_mask=attention_mask,\n",
    "#                 start_positions=start_positions,\n",
    "#                 end_positions=end_positions,\n",
    "#             )\n",
    "#             loss = outputs.loss\n",
    "\n",
    "#         scaler.scale(loss).backward()\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     avg_loss = total_loss / len(data_loader)\n",
    "#     print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss:.6f}\")\n",
    "\n",
    "# # Save the trained model\n",
    "# model.save_pretrained('../model/qa_greeting_model_mbert')\n",
    "# tokenizer.save_pretrained('../model/mbert_tokenizer')\n",
    "\n",
    "# # Clear GPU memory\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# print(\"Fine-tuning with mBERT completed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
