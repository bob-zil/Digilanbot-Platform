{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from tqdm import tqdm\n",
    "# import pickle\n",
    "\n",
    "# # Load dataset\n",
    "# df = pd.read_csv('final_data.csv')\n",
    "# df = df.dropna(subset=['text'])\n",
    "# df['text'] = df['text'].astype(str)\n",
    "\n",
    "# # Encode labels\n",
    "# label_encoder = LabelEncoder()\n",
    "# df['label'] = label_encoder.fit_transform(df['intent'])\n",
    "# pickle.dump(label_encoder, open('label_encoder.pkl', 'wb'))\n",
    "\n",
    "# # Tokenize text\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# X_encodings = tokenizer(df['text'].tolist(), truncation=True, padding=True, max_length=32, return_tensors='pt')\n",
    "# input_ids = X_encodings['input_ids']\n",
    "# attention_mask = X_encodings['attention_mask']\n",
    "# y = torch.tensor(df['label'].values)\n",
    "\n",
    "# # Split data\n",
    "# X_train_ids, X_test_ids, X_train_mask, X_test_mask, y_train, y_test = train_test_split(\n",
    "#     input_ids, attention_mask, y, test_size=0.2, random_state=42\n",
    "# )\n",
    "\n",
    "# # Create Dataset and DataLoader\n",
    "# class IntentDataset(Dataset):\n",
    "#     def __init__(self, input_ids, attention_mask, labels):\n",
    "#         self.input_ids = input_ids\n",
    "#         self.attention_mask = attention_mask\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return {\n",
    "#             'input_ids': self.input_ids[idx],\n",
    "#             'attention_mask': self.attention_mask[idx],\n",
    "#             'labels': self.labels[idx]\n",
    "#         }\n",
    "\n",
    "# train_dataset = IntentDataset(X_train_ids, X_train_mask, y_train)\n",
    "# test_dataset = IntentDataset(X_test_ids, X_test_mask, y_test)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# # Load model\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3).to(device)\n",
    "\n",
    "# # Loss and optimizer\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# # Train model\n",
    "# epochs = 5\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     train_loss = 0\n",
    "#     loop = tqdm(train_loader, leave=True)\n",
    "#     for batch in loop:\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         labels = batch['labels'].to(device)\n",
    "\n",
    "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#         loss = outputs.loss\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         train_loss += loss.item()\n",
    "#         loop.set_description(f'Epoch {epoch + 1}')\n",
    "#         loop.set_postfix(loss=loss.item())\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}, Loss: {train_loss / len(train_loader)}\")\n",
    "\n",
    "# # Validate model\n",
    "# model.eval()\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# with torch.no_grad():\n",
    "#     for batch in test_loader:\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         labels = batch['labels'].to(device)\n",
    "\n",
    "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         _, preds = torch.max(outputs.logits, dim=1)\n",
    "#         correct += (preds == labels).sum().item()\n",
    "#         total += labels.size(0)\n",
    "\n",
    "# accuracy = correct / total\n",
    "# print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# # import libraries\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "# from transformers import get_scheduler\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Check GPU availability\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# print(\"Device:\", device)\n",
    "\n",
    "# # Load dataset (replace with your actual dataset)\n",
    "# df = pd.read_csv(\"../data/final_data.csv\")  # Replace with your dataset file\n",
    "# df = df[['text', 'intent']]  # Ensure columns are named 'text' and 'label'\n",
    "\n",
    "# # Preprocess dataset (optional)\n",
    "# def preprocess_text(text):\n",
    "#     text = text.lower().strip()  # Lowercase and strip whitespace\n",
    "#     return text\n",
    "\n",
    "# df['text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# # Split into train and test\n",
    "# train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "#     df['text'], df['intent'], test_size=0.2, random_state=42\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prepare dataset for pytorch\n",
    "# class TextDataset(Dataset):\n",
    "#     def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "#         self.texts = texts\n",
    "#         self.labels = labels\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_len = max_len\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.texts)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         text = self.texts.iloc[idx]\n",
    "#         label = self.labels.iloc[idx]\n",
    "\n",
    "#         # Tokenize the text\n",
    "#         encoding = self.tokenizer(\n",
    "#             text,\n",
    "#             max_length=self.max_len,\n",
    "#             padding=\"max_length\",\n",
    "#             truncation=True,\n",
    "#             return_tensors=\"pt\"\n",
    "#         )\n",
    "\n",
    "#         return {\n",
    "#             \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "#             \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "#             \"label\": torch.tensor(label, dtype=torch.long)\n",
    "#         }\n",
    "\n",
    "# # Load tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# # Create datasets\n",
    "# train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
    "# test_dataset = TextDataset(test_texts, test_labels, tokenizer)\n",
    "\n",
    "# # Create DataLoaders\n",
    "# train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load pre-trained mBERT model\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     'bert-base-multilingual-cased',\n",
    "#     num_labels=len(df['label'].unique())  # Adjust to the number of classes\n",
    "# ).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define optimizer and scheduler\n",
    "# # Define optimizer\n",
    "# optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# # Define scheduler\n",
    "# num_training_steps = len(train_loader) * 5  # 5 epochs\n",
    "# lr_scheduler = get_scheduler(\n",
    "#     \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train model in Training loop\n",
    "# epochs = 5\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "#     for batch in progress_bar:\n",
    "#         input_ids = batch[\"input_ids\"].to(device)\n",
    "#         attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#         labels = batch[\"label\"].to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#         loss = outputs.loss\n",
    "\n",
    "#         # Backward pass\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         lr_scheduler.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "#         progress_bar.set_postfix({\"loss\": total_loss / len(train_loader)})\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate model\n",
    "# # Evaluation loop\n",
    "# def evaluate_model(model, data_loader):\n",
    "#     model.eval()\n",
    "#     all_labels = []\n",
    "#     all_preds = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch in data_loader:\n",
    "#             input_ids = batch[\"input_ids\"].to(device)\n",
    "#             attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#             labels = batch[\"label\"].to(device)\n",
    "\n",
    "#             outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#             preds = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "#             all_labels.extend(labels.cpu().numpy())\n",
    "#             all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "#     accuracy = accuracy_score(all_labels, all_preds)\n",
    "#     print(\"Accuracy:\", accuracy)\n",
    "#     print(\"Classification Report:\")\n",
    "#     print(classification_report(all_labels, all_preds))\n",
    "\n",
    "# # Evaluate the model\n",
    "# evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save model\n",
    "# model.save_pretrained(\"./mb_model\")\n",
    "# tokenizer.save_pretrained(\"./mb_model\")\n",
    "# print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\M\\miniconda3\\envs\\cudaenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used: cuda\n",
      "Number of unique intents: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M\\AppData\\Local\\Temp\\ipykernel_25304\\2073034326.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
      "C:\\Users\\M\\AppData\\Local\\Temp\\ipykernel_25304\\2073034326.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.attention_mask = torch.tensor(attention_mask, dtype=torch.long)\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\M\\miniconda3\\envs\\cudaenv\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Epoch 0: Loss = 1.2283, Accuracy = 44.44%\n",
      "Test Accuracy = 61.76%\n",
      "Epoch 2/5\n",
      "Epoch 1: Loss = 0.8706, Accuracy = 82.96%\n",
      "Test Accuracy = 91.18%\n",
      "Epoch 3/5\n",
      "Epoch 2: Loss = 0.5311, Accuracy = 97.04%\n",
      "Test Accuracy = 94.12%\n",
      "Epoch 4/5\n",
      "Epoch 3: Loss = 0.2837, Accuracy = 100.00%\n",
      "Test Accuracy = 94.12%\n",
      "Epoch 5/5\n",
      "Epoch 4: Loss = 0.1618, Accuracy = 99.26%\n",
      "Test Accuracy = 97.06%\n",
      "Model and tokenizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Ensure PyTorch runs on GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device used: {device}\")\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('../data/final_data.csv')  # Ensure it contains 'text' and 'intent' columns\n",
    "\n",
    "# Drop NaN values and convert text to strings\n",
    "df = df.dropna(subset=['text'])\n",
    "df['text'] = df['text'].astype(str)\n",
    "\n",
    "# Encode intent\n",
    "intent_encoder = LabelEncoder()\n",
    "df['intent'] = intent_encoder.fit_transform(df['intent'])\n",
    "pickle.dump(intent_encoder, open('intentencoder.pkl', 'wb'))  # Save encoder\n",
    "\n",
    "# Ensure intent are within the valid range\n",
    "num_intent = len(df['intent'].unique())\n",
    "print(f\"Number of unique intents: {num_intent}\")\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize text\n",
    "X = df['text'].tolist()\n",
    "y = df['intent'].values\n",
    "\n",
    "X_encodings = tokenizer(X, truncation=True, padding=True, max_length=32, return_tensors='pt')\n",
    "\n",
    "# Extract input_ids and attention_mask\n",
    "input_ids = X_encodings['input_ids']\n",
    "attention_mask = X_encodings['attention_mask']\n",
    "\n",
    "# Split data\n",
    "X_train_ids, X_test_ids, y_train, y_test = train_test_split(input_ids, y, test_size=0.2, random_state=42)\n",
    "X_train_mask, X_test_mask = train_test_split(attention_mask, test_size=0.2, random_state=42)\n",
    "\n",
    "# Custom Dataset class for PyTorch\n",
    "class IntentDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, intent):\n",
    "        self.input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "        self.attention_mask = torch.tensor(attention_mask, dtype=torch.long)\n",
    "        self.intent = torch.tensor(intent, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.intent)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'intent': self.intent[idx]\n",
    "        }\n",
    "\n",
    "# Create PyTorch DataLoader\n",
    "train_dataset = IntentDataset(X_train_ids, X_train_mask, y_train)\n",
    "test_dataset = IntentDataset(X_test_ids, X_test_mask, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_intent)\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Define training loop\n",
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        intent = batch['intent'].to(device)\n",
    "\n",
    "        # Corrected model call\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=intent)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(logits, dim=1)\n",
    "        total += intent.size(0)\n",
    "        correct += (predicted == intent).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Epoch {epoch}: Loss = {avg_loss:.4f}, Accuracy = {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Define evaluation loop\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            intent = batch['intent'].to(device)\n",
    "\n",
    "            # Corrected model call\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            _, predicted = torch.max(logits, dim=1)\n",
    "            total += intent.size(0)\n",
    "            correct += (predicted == intent).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy = {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Train the model for 5 epochs\n",
    "for epoch in range(5):\n",
    "    print(f\"Epoch {epoch + 1}/5\")\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    evaluate(model, test_loader)\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained('../model/intent_classifier_bert')\n",
    "tokenizer.save_pretrained('../model/bert_tokenizer')\n",
    "print(\"Model and tokenizer saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: [0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique labels:\", np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
